{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85386,"databundleVersionId":10252141,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-30T06:49:10.445628Z","iopub.execute_input":"2024-11-30T06:49:10.446081Z","iopub.status.idle":"2024-11-30T06:49:11.647693Z","shell.execute_reply.started":"2024-11-30T06:49:10.446044Z","shell.execute_reply":"2024-11-30T06:49:11.646470Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/icassp-person-in-bed-track-1/sample_submission.csv\n/kaggle/input/icassp-person-in-bed-track-1/train.json\n/kaggle/input/icassp-person-in-bed-track-1/License Agreement.pdf\n/kaggle/input/icassp-person-in-bed-track-1/test.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Read csv","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwith open('/kaggle/input/icassp-person-in-bed-track-1/train.json', 'r') as f:\n    data = json.load(f)\n\n\ndf = pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2024-11-30T08:45:07.430696Z","iopub.execute_input":"2024-11-30T08:45:07.431079Z","iopub.status.idle":"2024-11-30T08:45:52.344249Z","shell.execute_reply.started":"2024-11-30T08:45:07.431051Z","shell.execute_reply":"2024-11-30T08:45:52.342951Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\n\n# Define a function to create a low-pass Butterworth filter\ndef butter_lowpass(cutoff, fs, order=4):\n    nyquist = 0.5 * fs  # Nyquist frequency\n    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\n# Apply the low-pass filter to data\ndef butter_lowpass_filter(data, cutoff, fs, order=4):\n    b, a = butter_lowpass(cutoff, fs, order)\n    return filtfilt(b, a, data)\n\n# Function to process a single subject\ndef process_subject(subject_data, cutoff_freq=45.0, sampling_rate=250):\n    # Extract accelerometer data and timestamps\n    accel_data = np.array(subject_data['accel'])\n    ts_data = np.array(subject_data['ts'])\n    \n    # Apply low-pass filter to each axis (x, y, z)\n    filtered_accel = np.array([butter_lowpass_filter(accel_data[:, i], cutoff_freq, sampling_rate) for i in range(3)]).T\n    \n    # Return processed subject data without downsampling\n    return {\n        'subject': subject_data['subject'],\n        'accel': filtered_accel.tolist(),\n        'ts': ts_data.tolist(),\n        'labels': subject_data['label']  # Keep original labels\n    }\n\n# Load the JSON file\nwith open('/kaggle/input/icassp-person-in-bed-track-1/train.json', 'r') as f:\n    data = json.load(f)\n\n# Process each subject independently\nprocessed_data = [process_subject(subject_data) for subject_data in data]\n\n# Save the processed data back to JSON\nwith open('LowPass_train_45.json', 'w') as f:\n    json.dump(processed_data, f)\n\nprint(\"Low-pass filtering (15 Hz) complete without downsampling.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:00:51.690951Z","iopub.execute_input":"2024-11-30T07:00:51.692314Z","iopub.status.idle":"2024-11-30T07:03:52.117243Z","shell.execute_reply.started":"2024-11-30T07:00:51.692273Z","shell.execute_reply":"2024-11-30T07:03:52.116081Z"}},"outputs":[{"name":"stdout","text":"Low-pass filtering (15 Hz) complete without downsampling.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"len(df),df.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-30T08:46:06.435818Z","iopub.execute_input":"2024-11-30T08:46:06.436715Z","iopub.status.idle":"2024-11-30T08:46:06.446210Z","shell.execute_reply.started":"2024-11-30T08:46:06.436655Z","shell.execute_reply":"2024-11-30T08:46:06.444986Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(5911, (5911, 5))"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Split the DataFrame\ndf1 = df.iloc[:4728]  # First 4728 rows\ndf2 = df.iloc[4728:]  # Remaining rows","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:57:27.196371Z","iopub.execute_input":"2024-11-30T08:57:27.196771Z","iopub.status.idle":"2024-11-30T08:57:27.202159Z","shell.execute_reply.started":"2024-11-30T08:57:27.196740Z","shell.execute_reply":"2024-11-30T08:57:27.201042Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df1.shape,df2.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:07:45.493842Z","iopub.execute_input":"2024-11-30T09:07:45.494223Z","iopub.status.idle":"2024-11-30T09:07:45.501112Z","shell.execute_reply.started":"2024-11-30T09:07:45.494188Z","shell.execute_reply":"2024-11-30T09:07:45.499925Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"((4728, 5), (1183, 5))"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"4728","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-30T08:46:07.220312Z","iopub.execute_input":"2024-11-30T08:46:07.220840Z","iopub.status.idle":"2024-11-30T08:46:07.371761Z","shell.execute_reply.started":"2024-11-30T08:46:07.220790Z","shell.execute_reply":"2024-11-30T08:46:07.370533Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   subject  chunk_id                                                 ts  \\\n0       80         0  [1725411020.0856903, 1725411020.0897202, 17254...   \n1       14         1  [1725496482.262797, 1725496482.266831, 1725496...   \n2      300         2  [1726611768.779811, 1726611768.783809, 1726611...   \n3      193         3  [1725492595.229943, 1725492595.233981, 1725492...   \n4        9         4  [1725486791.803703, 1725486791.807745, 1725486...   \n\n                                               accel  label  \n0  [[-82.5083999633789, -56.03909683227539, 1000....      0  \n1  [[-20.9507999420166, 65.57850646972656, 1027.9...      1  \n2  [[108.17430114746094, 173.58509826660156, 968....      1  \n3  [[15.732599258422852, 70.74210357666016, 1001....      1  \n4  [[-29.815500259399414, 47.252403259277344, 100...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>chunk_id</th>\n      <th>ts</th>\n      <th>accel</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80</td>\n      <td>0</td>\n      <td>[1725411020.0856903, 1725411020.0897202, 17254...</td>\n      <td>[[-82.5083999633789, -56.03909683227539, 1000....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>1</td>\n      <td>[1725496482.262797, 1725496482.266831, 1725496...</td>\n      <td>[[-20.9507999420166, 65.57850646972656, 1027.9...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>2</td>\n      <td>[1726611768.779811, 1726611768.783809, 1726611...</td>\n      <td>[[108.17430114746094, 173.58509826660156, 968....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>193</td>\n      <td>3</td>\n      <td>[1725492595.229943, 1725492595.233981, 1725492...</td>\n      <td>[[15.732599258422852, 70.74210357666016, 1001....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>4</td>\n      <td>[1725486791.803703, 1725486791.807745, 1725486...</td>\n      <td>[[-29.815500259399414, 47.252403259277344, 100...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df['ts'].iloc[:1],len(df['ts'].iloc[:1])\ncnt = 0\nfor i in df['ts'].iloc[:1]:\n    for j in i:\n        # print(j)\n        cnt += 1\nprint(\"cnt = \",cnt)\n\n# print(len(df['accel'].iloc[:1]))\n# cnt = 0\n# for i in df['accel'].iloc[:1]:\n#     for j in i:\n#         print(j)\n#         cnt += 1\n# print(\"cnt = \",cnt)","metadata":{"execution":{"iopub.status.busy":"2024-11-28T03:49:46.663490Z","iopub.execute_input":"2024-11-28T03:49:46.663915Z","iopub.status.idle":"2024-11-28T03:49:46.672243Z","shell.execute_reply.started":"2024-11-28T03:49:46.663878Z","shell.execute_reply":"2024-11-28T03:49:46.670959Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cnt =  2500\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df['accel'].iloc[:1]","metadata":{"execution":{"iopub.status.busy":"2024-10-13T08:00:25.330624Z","iopub.execute_input":"2024-10-13T08:00:25.331110Z","iopub.status.idle":"2024-10-13T08:00:25.363706Z","shell.execute_reply.started":"2024-10-13T08:00:25.331064Z","shell.execute_reply":"2024-10-13T08:00:25.362060Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"0    [[-82.5083999633789, -56.03909683227539, 1000....\nName: accel, dtype: object"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## FFT","metadata":{}},{"cell_type":"code","source":"def extract_periodic_features(accel_data):\n    # Convert to numpy array for calculations\n    accel_array = np.array(accel_data)\n    x, y, z = accel_array[:, 0], accel_array[:, 1], accel_array[:, 2]\n    \n    # Compute FFT\n    fft_x = np.fft.fft(x)\n    fft_y = np.fft.fft(y)\n    fft_z = np.fft.fft(z)\n    \n    # Compute the frequency bins\n    n = len(x)\n    freqs = np.fft.fftfreq(n)\n\n    # Calculate the amplitude spectrum\n    amplitude_x = np.abs(fft_x)\n    amplitude_y = np.abs(fft_y)\n    amplitude_z = np.abs(fft_z)\n\n    # Extract features (e.g., dominant frequency)\n    dominant_freq_x = freqs[np.argmax(amplitude_x[1:n//2])]  # Exclude the zero frequency\n    dominant_freq_y = freqs[np.argmax(amplitude_y[1:n//2])]\n    dominant_freq_z = freqs[np.argmax(amplitude_z[1:n//2])]\n    \n    return {\n        'dominant_freq_x': dominant_freq_x,\n        'dominant_freq_y': dominant_freq_y,\n        'dominant_freq_z': dominant_freq_z,\n        'mean_x': np.mean(x),\n        'mean_y': np.mean(y),\n        'mean_z': np.mean(z),\n    }","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_list = df['accel'].apply(extract_periodic_features).tolist()\nfeatures_df = pd.DataFrame(feature_list)\n\n# Combine features with labels\nfinal_df = pd.concat([features_df, df['label']], axis=1)\n\nprint(final_df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Wavelet Transformation","metadata":{}},{"cell_type":"markdown","source":"## 4 features","metadata":{}},{"cell_type":"code","source":"import pywt\nimport numpy as np\n\ndef extract_wavelet_features_old(accel_data, wavelet='db4', level=4):\n    \"\"\"\n    Extract features using wavelet transformation on accelerometer data.\n    \n    Parameters:\n        accel_data: list or numpy array of accelerometer data (Nx3)\n                   (where N is the number of samples, and 3 refers to x, y, z axes)\n        wavelet: Wavelet type to use (default: 'db4')\n        level: Decomposition level (default: 4)\n    \n    Returns:\n        A dictionary containing wavelet features for x, y, z axes.\n    \"\"\"\n    # Convert to numpy array for calculations\n    accel_array = np.array(accel_data)\n    x, y, z = accel_array[:, 0], accel_array[:, 1], accel_array[:, 2]\n\n    # Perform discrete wavelet transform for each axis\n    coeffs_x = pywt.wavedec(x, wavelet, level=level)\n    coeffs_y = pywt.wavedec(y, wavelet, level=level)\n    coeffs_z = pywt.wavedec(z, wavelet, level=level)\n\n    # Helper function to summarize wavelet coefficients\n    def summarize_coeffs(coeffs):\n        return {\n            'mean': np.mean(coeffs),\n            'std': np.std(coeffs),\n            'max': np.max(coeffs),\n            'min': np.min(coeffs),\n            'energy': np.sum(np.square(coeffs)),\n            'skewness': np.mean((coeffs - np.mean(coeffs))**3) / (np.std(coeffs)**3 + 1e-6),\n            'kurtosis': np.mean((coeffs - np.mean(coeffs))**4) / (np.std(coeffs)**4 + 1e-6),\n        }\n\n    # Extract features for approximation (cA) and detail (cD) coefficients\n    features = {}\n    for i, (cA_x, cA_y, cA_z) in enumerate(zip(coeffs_x, coeffs_y, coeffs_z)):\n        features.update({\n            f'cA_mean_x_level{i}': summarize_coeffs(cA_x)['mean'],\n            f'cA_std_x_level{i}': summarize_coeffs(cA_x)['std'],\n            f'cA_max_x_level{i}': summarize_coeffs(cA_x)['max'],\n            f'cA_energy_x_level{i}': summarize_coeffs(cA_x)['energy'],\n            f'cA_mean_y_level{i}': summarize_coeffs(cA_y)['mean'],\n            f'cA_std_y_level{i}': summarize_coeffs(cA_y)['std'],\n            f'cA_max_y_level{i}': summarize_coeffs(cA_y)['max'],\n            f'cA_energy_y_level{i}': summarize_coeffs(cA_y)['energy'],\n            f'cA_mean_z_level{i}': summarize_coeffs(cA_z)['mean'],\n            f'cA_std_z_level{i}': summarize_coeffs(cA_z)['std'],\n            f'cA_max_z_level{i}': summarize_coeffs(cA_z)['max'],\n            f'cA_energy_z_level{i}': summarize_coeffs(cA_z)['energy'],\n        })\n\n    return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T14:58:35.849310Z","iopub.execute_input":"2024-11-27T14:58:35.849785Z","iopub.status.idle":"2024-11-27T14:58:35.862311Z","shell.execute_reply.started":"2024-11-27T14:58:35.849715Z","shell.execute_reply":"2024-11-27T14:58:35.860937Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## More features","metadata":{}},{"cell_type":"code","source":"import pywt\nimport numpy as np\nfrom scipy.stats import skew, kurtosis, entropy\n\ndef extract_wavelet_features(accel_data, wavelet='db4', level=2):\n    \"\"\"\n    Extract features using wavelet transformation on accelerometer data.\n    \n    Parameters:\n        accel_data: list or numpy array of accelerometer data (Nx3)\n                   (where N is the number of samples, and 3 refers to x, y, z axes)\n        wavelet: Wavelet type to use (default: 'db4')\n        level: Decomposition level (default: 6)\n    \n    Returns:\n        A dictionary containing wavelet features for x, y, z axes.\n    \"\"\"\n    # Convert to numpy array for calculations\n    accel_array = np.array(accel_data)\n    x, y, z = accel_array[:, 0], accel_array[:, 1], accel_array[:, 2]\n\n    # Perform discrete wavelet transform for each axis\n    coeffs_x = pywt.wavedec(x, wavelet, level=level)\n    coeffs_y = pywt.wavedec(y, wavelet, level=level)\n    coeffs_z = pywt.wavedec(z, wavelet, level=level)\n\n    # Helper function to summarize wavelet coefficients\n    def summarize_coeffs(coeffs):\n        freqs = np.fft.fftfreq(len(coeffs))  # Frequency domain information\n        fft_coeffs = np.fft.fft(coeffs)      # FFT of the coefficients\n        amplitude_spectrum = np.abs(fft_coeffs)\n        dominant_freq = freqs[np.argmax(amplitude_spectrum[1:])] if len(freqs) > 1 else 0\n        \n        return {\n            'mean': np.mean(coeffs),\n            'std': np.std(coeffs),\n            'max': np.max(coeffs),\n            'min': np.min(coeffs),\n            'energy': np.sum(np.square(coeffs)),\n            'skewness': skew(coeffs),\n            'kurtosis': kurtosis(coeffs),\n            'median': np.median(coeffs),\n            'iqr': np.percentile(coeffs, 75) - np.percentile(coeffs, 25),\n            'entropy': entropy(np.abs(coeffs) + 1e-6),  # Add small value to avoid log(0)\n            'dominant_frequency': dominant_freq\n        }\n\n    # Extract features for approximation (cA) and detail (cD) coefficients\n    features = {}\n    for i, (cA_x, cA_y, cA_z) in enumerate(zip(coeffs_x, coeffs_y, coeffs_z)):\n        features.update({\n            f'cA_mean_x_level{i}': summarize_coeffs(cA_x)['mean'],\n            f'cA_std_x_level{i}': summarize_coeffs(cA_x)['std'],\n            f'cA_max_x_level{i}': summarize_coeffs(cA_x)['max'],\n            f'cA_min_x_level{i}': summarize_coeffs(cA_x)['min'],\n            f'cA_energy_x_level{i}': summarize_coeffs(cA_x)['energy'],\n            f'cA_skewness_x_level{i}': summarize_coeffs(cA_x)['skewness'],\n            f'cA_kurtosis_x_level{i}': summarize_coeffs(cA_x)['kurtosis'],\n            f'cA_median_x_level{i}': summarize_coeffs(cA_x)['median'],\n            f'cA_iqr_x_level{i}': summarize_coeffs(cA_x)['iqr'],\n            f'cA_entropy_x_level{i}': summarize_coeffs(cA_x)['entropy'],\n            f'cA_dominant_frequency_x_level{i}': summarize_coeffs(cA_x)['dominant_frequency'],\n\n            f'cA_mean_y_level{i}': summarize_coeffs(cA_y)['mean'],\n            f'cA_std_y_level{i}': summarize_coeffs(cA_y)['std'],\n            f'cA_max_y_level{i}': summarize_coeffs(cA_y)['max'],\n            f'cA_min_y_level{i}': summarize_coeffs(cA_y)['min'],\n            f'cA_energy_y_level{i}': summarize_coeffs(cA_y)['energy'],\n            f'cA_skewness_y_level{i}': summarize_coeffs(cA_y)['skewness'],\n            f'cA_kurtosis_y_level{i}': summarize_coeffs(cA_y)['kurtosis'],\n            f'cA_median_y_level{i}': summarize_coeffs(cA_y)['median'],\n            f'cA_iqr_y_level{i}': summarize_coeffs(cA_y)['iqr'],\n            f'cA_entropy_y_level{i}': summarize_coeffs(cA_y)['entropy'],\n            f'cA_dominant_frequency_y_level{i}': summarize_coeffs(cA_y)['dominant_frequency'],\n\n            f'cA_mean_z_level{i}': summarize_coeffs(cA_z)['mean'],\n            f'cA_std_z_level{i}': summarize_coeffs(cA_z)['std'],\n            f'cA_max_z_level{i}': summarize_coeffs(cA_z)['max'],\n            f'cA_min_z_level{i}': summarize_coeffs(cA_z)['min'],\n            f'cA_energy_z_level{i}': summarize_coeffs(cA_z)['energy'],\n            f'cA_skewness_z_level{i}': summarize_coeffs(cA_z)['skewness'],\n            f'cA_kurtosis_z_level{i}': summarize_coeffs(cA_z)['kurtosis'],\n            f'cA_median_z_level{i}': summarize_coeffs(cA_z)['median'],\n            f'cA_iqr_z_level{i}': summarize_coeffs(cA_z)['iqr'],\n            f'cA_entropy_z_level{i}': summarize_coeffs(cA_z)['entropy'],\n            f'cA_dominant_frequency_z_level{i}': summarize_coeffs(cA_z)['dominant_frequency'],\n        })\n\n    return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:14:14.313600Z","iopub.execute_input":"2024-11-28T16:14:14.314707Z","iopub.status.idle":"2024-11-28T16:14:14.331433Z","shell.execute_reply.started":"2024-11-28T16:14:14.314664Z","shell.execute_reply":"2024-11-28T16:14:14.330346Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Both approx and detial components","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pywt\nfrom scipy.stats import skew, kurtosis\nfrom scipy.stats import entropy\n\ndef extract_wavelet_features(accel_data, wavelet='db4', level=2):\n    \"\"\"\n    Extract features using wavelet transformation on accelerometer data.\n    \n    Parameters:\n        accel_data: list or numpy array of accelerometer data (Nx3)\n                   (where N is the number of samples, and 3 refers to x, y, z axes)\n        wavelet: Wavelet type to use (default: 'db4')\n        level: Decomposition level (default: 6)\n    \n    Returns:\n        A dictionary containing wavelet features for x, y, z axes.\n    \"\"\"\n    # Convert to numpy array for calculations\n    accel_array = np.array(accel_data)\n    x, y, z = accel_array[:, 0], accel_array[:, 1], accel_array[:, 2]\n\n    # Perform discrete wavelet transform for each axis\n    coeffs_x = pywt.wavedec(x, wavelet, level=level)\n    coeffs_y = pywt.wavedec(y, wavelet, level=level)\n    coeffs_z = pywt.wavedec(z, wavelet, level=level)\n\n    # Helper function to summarize wavelet coefficients\n    def summarize_coeffs(coeffs):\n        freqs = np.fft.fftfreq(len(coeffs))  # Frequency domain information\n        fft_coeffs = np.fft.fft(coeffs)      # FFT of the coefficients\n        amplitude_spectrum = np.abs(fft_coeffs)\n        dominant_freq = freqs[np.argmax(amplitude_spectrum[1:])] if len(freqs) > 1 else 0\n        \n        return {\n            'mean': np.mean(coeffs),\n            'std': np.std(coeffs),\n            'max': np.max(coeffs),\n            'min': np.min(coeffs),\n            'energy': np.sum(np.square(coeffs)),\n            'skewness': skew(coeffs),\n            'kurtosis': kurtosis(coeffs),\n            'median': np.median(coeffs),\n            'iqr': np.percentile(coeffs, 75) - np.percentile(coeffs, 25),\n            'entropy': entropy(np.abs(coeffs) + 1e-6),  # Add small value to avoid log(0)\n            'dominant_frequency': dominant_freq\n        }\n\n    # Extract features for both approximation (cA) and detail (cD) coefficients\n    features = {}\n    for i, (coeff_x, coeff_y, coeff_z) in enumerate(zip(coeffs_x, coeffs_y, coeffs_z)):\n        # Determine component type: cA (approximation) or cD (detail)\n        component = 'cA' if i == 0 else f'cD{i}'\n\n        # Extract features for each axis and component\n        for axis, coeffs in zip(['x', 'y', 'z'], [coeff_x, coeff_y, coeff_z]):\n            summary = summarize_coeffs(coeffs)\n            features.update({\n                f'{component}_mean_{axis}_level{i}': summary['mean'],\n                f'{component}_std_{axis}_level{i}': summary['std'],\n                f'{component}_max_{axis}_level{i}': summary['max'],\n                f'{component}_min_{axis}_level{i}': summary['min'],\n                f'{component}_energy_{axis}_level{i}': summary['energy'],\n                f'{component}_skewness_{axis}_level{i}': summary['skewness'],\n                f'{component}_kurtosis_{axis}_level{i}': summary['kurtosis'],\n                f'{component}_median_{axis}_level{i}': summary['median'],\n                f'{component}_iqr_{axis}_level{i}': summary['iqr'],\n                f'{component}_entropy_{axis}_level{i}': summary['entropy'],\n                f'{component}_dominant_frequency_{axis}_level{i}': summary['dominant_frequency'],\n            })\n\n    return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:46:57.922483Z","iopub.execute_input":"2024-11-30T08:46:57.923663Z","iopub.status.idle":"2024-11-30T08:46:58.021983Z","shell.execute_reply.started":"2024-11-30T08:46:57.923623Z","shell.execute_reply":"2024-11-30T08:46:58.020712Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Wavelet Packet Transformation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pywt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Example accelerometer data (Nx3 for x, y, z axes)\ndef generate_dummy_data(n_samples, n_features=3):\n    np.random.seed(42)\n    accel_data = [np.random.randn(n_samples, n_features) for _ in range(100)]\n    labels = np.random.choice([0, 1], size=100)  # Binary labels\n    return accel_data, labels\n\n# Function to perform wavelet packet decomposition and extract features\ndef extract_wavelet_packet_features(accel_data, wavelet='db4', max_level=2):\n    \"\"\"\n    Extract features using wavelet packet decomposition.\n    \"\"\"\n    accel_array = np.array(accel_data)\n    x, y, z = accel_array[:, 0], accel_array[:, 1], accel_array[:, 2]\n\n    def decompose_and_extract(axis_data):\n        wp = pywt.WaveletPacket(data=axis_data, wavelet=wavelet, maxlevel=max_level)\n        features = {}\n        \n        # Iterate over all nodes in the wavelet packet tree\n        for node in wp.get_level(max_level, order='natural'):\n            node_data = node.data\n            features[f'{node.path}_mean'] = np.mean(node_data)\n            features[f'{node.path}_std'] = np.std(node_data)\n            features[f'{node.path}_energy'] = np.sum(np.square(node_data))\n        return features\n\n    # Extract features for each axis\n    features_x = decompose_and_extract(x)\n    features_y = decompose_and_extract(y)\n    features_z = decompose_and_extract(z)\n    \n    # Combine features from all axes\n    features = {**features_x, **features_y, **features_z}\n    return features\n\n# Evaluate the impact of unequal splits on accuracy\ndef evaluate_unequal_splits(accel_data, labels, wavelet='db4', max_level_range=[2, 3, 4]):\n    results = []\n    \n    for max_level in max_level_range:\n        features = [extract_wavelet_packet_features(data, wavelet, max_level) for data in accel_data]\n        feature_df = pd.DataFrame(features)\n        \n        # Train/test split\n        X_train, X_test, y_train, y_test = train_test_split(feature_df, labels, test_size=0.3, random_state=42)\n        \n        # Train a simple Random Forest Classifier\n        model = RandomForestClassifier(random_state=42)\n        model.fit(X_train, y_train)\n        \n        # Evaluate accuracy\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        results.append({'max_level': max_level, 'accuracy': accuracy})\n    \n    return pd.DataFrame(results)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T06:28:31.409398Z","iopub.execute_input":"2024-11-28T06:28:31.409803Z","iopub.status.idle":"2024-11-28T06:28:31.422755Z","shell.execute_reply.started":"2024-11-28T06:28:31.409767Z","shell.execute_reply":"2024-11-28T06:28:31.421251Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"# Calling wavelet","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwith open('/kaggle/working/LowPass_train_45.json', 'r') as f:\n    data = json.load(f)\n\n\ndf = pd.DataFrame(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:05:43.916832Z","iopub.execute_input":"2024-11-30T07:05:43.917232Z","iopub.status.idle":"2024-11-30T07:06:29.675835Z","shell.execute_reply.started":"2024-11-30T07:05:43.917198Z","shell.execute_reply":"2024-11-30T07:06:29.674798Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T08:49:39.129001Z","iopub.execute_input":"2024-11-30T08:49:39.129362Z","iopub.status.idle":"2024-11-30T08:49:39.252161Z","shell.execute_reply.started":"2024-11-30T08:49:39.129333Z","shell.execute_reply":"2024-11-30T08:49:39.251081Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   subject  chunk_id                                                 ts  \\\n0       80         0  [1725411020.0856903, 1725411020.0897202, 17254...   \n1       14         1  [1725496482.262797, 1725496482.266831, 1725496...   \n2      300         2  [1726611768.779811, 1726611768.783809, 1726611...   \n3      193         3  [1725492595.229943, 1725492595.233981, 1725492...   \n4        9         4  [1725486791.803703, 1725486791.807745, 1725486...   \n\n                                               accel  label  \n0  [[-82.5083999633789, -56.03909683227539, 1000....      0  \n1  [[-20.9507999420166, 65.57850646972656, 1027.9...      1  \n2  [[108.17430114746094, 173.58509826660156, 968....      1  \n3  [[15.732599258422852, 70.74210357666016, 1001....      1  \n4  [[-29.815500259399414, 47.252403259277344, 100...      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>chunk_id</th>\n      <th>ts</th>\n      <th>accel</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>80</td>\n      <td>0</td>\n      <td>[1725411020.0856903, 1725411020.0897202, 17254...</td>\n      <td>[[-82.5083999633789, -56.03909683227539, 1000....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>1</td>\n      <td>[1725496482.262797, 1725496482.266831, 1725496...</td>\n      <td>[[-20.9507999420166, 65.57850646972656, 1027.9...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>2</td>\n      <td>[1726611768.779811, 1726611768.783809, 1726611...</td>\n      <td>[[108.17430114746094, 173.58509826660156, 968....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>193</td>\n      <td>3</td>\n      <td>[1725492595.229943, 1725492595.233981, 1725492...</td>\n      <td>[[15.732599258422852, 70.74210357666016, 1001....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>4</td>\n      <td>[1725486791.803703, 1725486791.807745, 1725486...</td>\n      <td>[[-29.815500259399414, 47.252403259277344, 100...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"features_df2.shape,len(df2['label']),final_df2.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:14:06.276772Z","iopub.execute_input":"2024-11-30T09:14:06.277177Z","iopub.status.idle":"2024-11-30T09:14:06.285925Z","shell.execute_reply.started":"2024-11-30T09:14:06.277135Z","shell.execute_reply":"2024-11-30T09:14:06.284705Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"((1183, 99), 1183, (1183, 100))"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"final_df2 = pd.concat([features_df2.reset_index(drop=True), df2['label'].reset_index(drop=True)], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:14:03.733797Z","iopub.execute_input":"2024-11-30T09:14:03.734206Z","iopub.status.idle":"2024-11-30T09:14:03.741365Z","shell.execute_reply.started":"2024-11-30T09:14:03.734163Z","shell.execute_reply":"2024-11-30T09:14:03.740219Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"feature_list = df['accel'].apply(extract_wavelet_features).tolist()\n# feature_list1 = df1['accel'].apply(extract_wavelet_features).tolist()\n# feature_list2 = df2['accel'].apply(extract_wavelet_features).tolist()\n\n# features_df1 = pd.DataFrame(feature_list1)\n# features_df2 = pd.DataFrame(feature_list2)\n\n# Combine features with labels\nfinal_df = pd.concat([features_df, df['label']], axis=1)\n# final_df1 = pd.concat([features_df1, df1['label']], axis=1)\n# final_df2 = pd.concat([features_df2, df2['label']], axis=1)\n\n# final_df = pd.concat([features_df, df['labels']], axis=1)\n\nprint(final_df.head(),final_df.shape)\n# print(final_df1.head(),final_df1.shape)\n# print(final_df2.head(),final_df2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:16:20.265899Z","iopub.execute_input":"2024-11-30T09:16:20.266459Z","iopub.status.idle":"2024-11-30T09:18:27.790990Z","shell.execute_reply.started":"2024-11-30T09:16:20.266405Z","shell.execute_reply":"2024-11-30T09:18:27.789741Z"}},"outputs":[{"name":"stdout","text":"   cA_mean_x_level0  cA_std_x_level0  cA_max_x_level0  cA_min_x_level0  \\\n0       -164.618457         0.604509      -162.780599      -166.840873   \n1       -313.811041       122.033092        58.848554      -664.304205   \n2        218.086563         1.702307       223.668579       214.183751   \n3         32.078675         0.729502        34.458902        29.929647   \n4        -60.829468         1.212052       -55.629101       -65.011295   \n\n   cA_energy_x_level0  cA_skewness_x_level0  cA_kurtosis_x_level0  \\\n0        1.707275e+07             -0.076523             -0.001264   \n1        7.142275e+07             -1.054212              2.516957   \n2        2.996573e+07              0.572028              0.277138   \n3        6.486314e+05              0.004732              0.795969   \n4        2.332067e+06              0.026571              0.925110   \n\n   cA_median_x_level0  cA_iqr_x_level0  cA_entropy_x_level0  ...  \\\n0         -164.616285         0.847286             6.445713  ...   \n1         -281.868283        11.314707             6.371154  ...   \n2          217.849945         2.077938             6.445689  ...   \n3           32.064042         0.851680             6.445461  ...   \n4          -60.834117         1.367675             6.445521  ...   \n\n   cD2_max_z_level2  cD2_min_z_level2  cD2_energy_z_level2  \\\n0          0.819996         -0.818638            31.416937   \n1         10.556062        -15.098135          2800.118615   \n2          1.298925         -1.329294            71.135769   \n3          1.320818         -0.725052            67.260343   \n4         10.190870         -8.580268           676.981607   \n\n   cD2_skewness_z_level2  cD2_kurtosis_z_level2  cD2_median_z_level2  \\\n0              -0.042555               1.559825            -0.000317   \n1              -1.311342              23.768540            -0.003193   \n2               0.081944               3.606246             0.000816   \n3               0.606025               2.351809             0.016742   \n4               1.018494              52.391830             0.000688   \n\n   cD2_iqr_z_level2  cD2_entropy_z_level2  cD2_dominant_frequency_z_level2  \\\n0          0.209011              6.840234                         0.457302   \n1          0.358169              6.003871                         0.314445   \n2          0.268680              6.782070                        -0.433360   \n3          0.285238              6.812064                         0.013567   \n4          0.361808              6.383362                        -0.489226   \n\n   label  \n0      0  \n1      1  \n2      1  \n3      1  \n4      0  \n\n[5 rows x 100 columns] (5911, 100)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(final_df.head(),final_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:07:12.255705Z","iopub.execute_input":"2024-11-30T09:07:12.256228Z","iopub.status.idle":"2024-11-30T09:07:12.274540Z","shell.execute_reply.started":"2024-11-30T09:07:12.256180Z","shell.execute_reply":"2024-11-30T09:07:12.273196Z"}},"outputs":[{"name":"stdout","text":"   cA_mean_x_level0  cA_std_x_level0  cA_max_x_level0  cA_min_x_level0  \\\n0       -164.618457         0.604509      -162.780599      -166.840873   \n1       -313.811041       122.033092        58.848554      -664.304205   \n2        218.086563         1.702307       223.668579       214.183751   \n3         32.078675         0.729502        34.458902        29.929647   \n4        -60.829468         1.212052       -55.629101       -65.011295   \n\n   cA_energy_x_level0  cA_skewness_x_level0  cA_kurtosis_x_level0  \\\n0        1.707275e+07             -0.076523             -0.001264   \n1        7.142275e+07             -1.054212              2.516957   \n2        2.996573e+07              0.572028              0.277138   \n3        6.486314e+05              0.004732              0.795969   \n4        2.332067e+06              0.026571              0.925110   \n\n   cA_median_x_level0  cA_iqr_x_level0  cA_entropy_x_level0  ...  \\\n0         -164.616285         0.847286             6.445713  ...   \n1         -281.868283        11.314707             6.371154  ...   \n2          217.849945         2.077938             6.445689  ...   \n3           32.064042         0.851680             6.445461  ...   \n4          -60.834117         1.367675             6.445521  ...   \n\n   cD2_max_z_level2  cD2_min_z_level2  cD2_energy_z_level2  \\\n0          0.819996         -0.818638            31.416937   \n1         10.556062        -15.098135          2800.118615   \n2          1.298925         -1.329294            71.135769   \n3          1.320818         -0.725052            67.260343   \n4         10.190870         -8.580268           676.981607   \n\n   cD2_skewness_z_level2  cD2_kurtosis_z_level2  cD2_median_z_level2  \\\n0              -0.042555               1.559825            -0.000317   \n1              -1.311342              23.768540            -0.003193   \n2               0.081944               3.606246             0.000816   \n3               0.606025               2.351809             0.016742   \n4               1.018494              52.391830             0.000688   \n\n   cD2_iqr_z_level2  cD2_entropy_z_level2  cD2_dominant_frequency_z_level2  \\\n0          0.209011              6.840234                         0.457302   \n1          0.358169              6.003871                         0.314445   \n2          0.268680              6.782070                        -0.433360   \n3          0.285238              6.812064                         0.013567   \n4          0.361808              6.383362                        -0.489226   \n\n   label  \n0      0  \n1      1  \n2      1  \n3      1  \n4      0  \n\n[5 rows x 100 columns] (2366, 100)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Train RFC","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n\nX_train = final_df1.drop(columns='label')\ny_train = final_df1['label']\nX_test = final_df2.drop(columns='label')\ny_test = final_df2['label']\n\n# Split the dataset into features and labels\n# X = final_df.drop(columns='labels')\n# y = final_df['labels']\n\n# Split into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4,stratify=y)\n\n# Step 3: Scale Features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and train the model\nmodel = RandomForestClassifier(random_state=42)\n# model.fit(X_train, y_train)\nmodel.fit(X_train,y_train)\n# model.fit(X, y)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Calculate and print the balanced accuracy\nbalanced_acc = balanced_accuracy_score(y_test, y_pred)\nprint(f\"Balanced Accuracy: {balanced_acc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:23:20.266254Z","iopub.execute_input":"2024-11-30T09:23:20.266921Z","iopub.status.idle":"2024-11-30T09:23:23.518072Z","shell.execute_reply.started":"2024-11-30T09:23:20.266884Z","shell.execute_reply":"2024-11-30T09:23:23.516893Z"}},"outputs":[{"name":"stdout","text":"[[431   4]\n [  0 748]]\n              precision    recall  f1-score   support\n\n           0       1.00      0.99      1.00       435\n           1       0.99      1.00      1.00       748\n\n    accuracy                           1.00      1183\n   macro avg       1.00      1.00      1.00      1183\nweighted avg       1.00      1.00      1.00      1183\n\nBalanced Accuracy: 0.9954022988505746\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Train ANN","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Step 1: Generate Synthetic Data\n# 2 features and binary labels (0 or 1)\n# np.random.seed(42)\n# X = final_df.drop(columns='label')\n# y = final_df['label']\n\n# Step 2: Split Data into Train/Test\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = final_df1.drop(columns='label')\ny_train = final_df1['label']\nX_test = final_df2.drop(columns='label')\ny_test = final_df2['label']\n\n# Step 3: Scale Features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Step 4: Build the ANN Model\nann_model = Sequential([\n    Dense(32, input_dim=X_train.shape[1], activation='relu'),  # Input layer with 32 neurons\n    Dropout(0.2),  # Dropout for regularization\n    Dense(16, activation='relu'),  # Hidden layer with 16 neurons\n    Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification\n])\n\n# Step 5: Compile the Model\nann_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Step 6: Train the Model\nhistory = ann_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n\n# Step 7: Evaluate the Model\ny_pred = (ann_model.predict(X_test) > 0.5).astype(int)  # Predict and convert probabilities to binary classes\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T09:24:55.390276Z","iopub.execute_input":"2024-11-30T09:24:55.390676Z","iopub.status.idle":"2024-11-30T09:25:10.527999Z","shell.execute_reply.started":"2024-11-30T09:24:55.390643Z","shell.execute_reply":"2024-11-30T09:25:10.526669Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6305 - loss: 0.6148 - val_accuracy: 0.8266 - val_loss: 0.4162\nEpoch 2/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8479 - loss: 0.3723 - val_accuracy: 0.8605 - val_loss: 0.3318\nEpoch 3/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8852 - loss: 0.2827 - val_accuracy: 0.8732 - val_loss: 0.2957\nEpoch 4/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9062 - loss: 0.2351 - val_accuracy: 0.8837 - val_loss: 0.2663\nEpoch 5/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9090 - loss: 0.2203 - val_accuracy: 0.8996 - val_loss: 0.2518\nEpoch 6/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9240 - loss: 0.1855 - val_accuracy: 0.9070 - val_loss: 0.2397\nEpoch 7/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9371 - loss: 0.1606 - val_accuracy: 0.9218 - val_loss: 0.2312\nEpoch 8/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9388 - loss: 0.1446 - val_accuracy: 0.9239 - val_loss: 0.2219\nEpoch 9/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9499 - loss: 0.1260 - val_accuracy: 0.9313 - val_loss: 0.2140\nEpoch 10/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1215 - val_accuracy: 0.9313 - val_loss: 0.2186\nEpoch 11/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9554 - loss: 0.1312 - val_accuracy: 0.9440 - val_loss: 0.2096\nEpoch 12/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.1181 - val_accuracy: 0.9461 - val_loss: 0.2108\nEpoch 13/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9563 - loss: 0.1180 - val_accuracy: 0.9482 - val_loss: 0.2086\nEpoch 14/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9643 - loss: 0.1015 - val_accuracy: 0.9471 - val_loss: 0.2122\nEpoch 15/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.0936 - val_accuracy: 0.9461 - val_loss: 0.2155\nEpoch 16/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9704 - loss: 0.0919 - val_accuracy: 0.9461 - val_loss: 0.2148\nEpoch 17/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.0950 - val_accuracy: 0.9419 - val_loss: 0.2177\nEpoch 18/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9797 - loss: 0.0702 - val_accuracy: 0.9503 - val_loss: 0.2286\nEpoch 19/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.0827 - val_accuracy: 0.9514 - val_loss: 0.2231\nEpoch 20/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9706 - loss: 0.0763 - val_accuracy: 0.9503 - val_loss: 0.2257\nEpoch 21/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9717 - loss: 0.0826 - val_accuracy: 0.9514 - val_loss: 0.2196\nEpoch 22/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.0785 - val_accuracy: 0.9493 - val_loss: 0.2104\nEpoch 23/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0802 - val_accuracy: 0.9535 - val_loss: 0.2171\nEpoch 24/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9795 - loss: 0.0697 - val_accuracy: 0.9482 - val_loss: 0.2256\nEpoch 25/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9759 - loss: 0.0714 - val_accuracy: 0.9503 - val_loss: 0.2228\nEpoch 26/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9718 - loss: 0.0841 - val_accuracy: 0.9535 - val_loss: 0.2355\nEpoch 27/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9729 - loss: 0.0671 - val_accuracy: 0.9545 - val_loss: 0.2418\nEpoch 28/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.0657 - val_accuracy: 0.9535 - val_loss: 0.2406\nEpoch 29/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9722 - loss: 0.0630 - val_accuracy: 0.9514 - val_loss: 0.2357\nEpoch 30/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9801 - loss: 0.0572 - val_accuracy: 0.9503 - val_loss: 0.2414\nEpoch 31/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9786 - loss: 0.0601 - val_accuracy: 0.9503 - val_loss: 0.2464\nEpoch 32/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9783 - loss: 0.0598 - val_accuracy: 0.9493 - val_loss: 0.2556\nEpoch 33/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0456 - val_accuracy: 0.9482 - val_loss: 0.2632\nEpoch 34/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9767 - loss: 0.0649 - val_accuracy: 0.9514 - val_loss: 0.2577\nEpoch 35/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0592 - val_accuracy: 0.9535 - val_loss: 0.2643\nEpoch 36/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9845 - loss: 0.0473 - val_accuracy: 0.9545 - val_loss: 0.2588\nEpoch 37/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9879 - loss: 0.0445 - val_accuracy: 0.9535 - val_loss: 0.2605\nEpoch 38/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0444 - val_accuracy: 0.9556 - val_loss: 0.2614\nEpoch 39/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0447 - val_accuracy: 0.9545 - val_loss: 0.2570\nEpoch 40/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0478 - val_accuracy: 0.9545 - val_loss: 0.2673\nEpoch 41/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0513 - val_accuracy: 0.9545 - val_loss: 0.2842\nEpoch 42/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0346 - val_accuracy: 0.9556 - val_loss: 0.2815\nEpoch 43/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0440 - val_accuracy: 0.9545 - val_loss: 0.2656\nEpoch 44/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9904 - loss: 0.0338 - val_accuracy: 0.9545 - val_loss: 0.2888\nEpoch 45/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0384 - val_accuracy: 0.9556 - val_loss: 0.2915\nEpoch 46/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0351 - val_accuracy: 0.9556 - val_loss: 0.2881\nEpoch 47/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9823 - loss: 0.0471 - val_accuracy: 0.9514 - val_loss: 0.3040\nEpoch 48/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0362 - val_accuracy: 0.9545 - val_loss: 0.3110\nEpoch 49/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0361 - val_accuracy: 0.9545 - val_loss: 0.3088\nEpoch 50/50\n\u001b[1m119/119\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0317 - val_accuracy: 0.9524 - val_loss: 0.3226\n\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96       435\n           1       0.98      0.97      0.98       748\n\n    accuracy                           0.97      1183\n   macro avg       0.96      0.97      0.97      1183\nweighted avg       0.97      0.97      0.97      1183\n\nAccuracy: 0.9687235841081995\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwith open('/kaggle/input/icassp-person-in-bed-track-1/test.json', 'r') as f:\n    data_test = json.load(f)\n\n\ndf_test = pd.DataFrame(data_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:11:56.758792Z","iopub.execute_input":"2024-11-30T07:11:56.759296Z","iopub.status.idle":"2024-11-30T07:12:07.358750Z","shell.execute_reply.started":"2024-11-30T07:11:56.759258Z","shell.execute_reply":"2024-11-30T07:12:07.357750Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:12:29.764412Z","iopub.execute_input":"2024-11-30T07:12:29.764822Z","iopub.status.idle":"2024-11-30T07:12:29.893127Z","shell.execute_reply.started":"2024-11-30T07:12:29.764785Z","shell.execute_reply":"2024-11-30T07:12:29.892002Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   subject  chunk_id                                              accel  \\\n0       91         0  [[-16.750499725341797, 221.14559936523438, 978...   \n1       91         1  [[12.323999404907227, -6.38040018081665, 1004....   \n2       91         2  [[39.14820098876953, 99.29010009765625, 989.72...   \n3       91         3  [[-19.905601501464844, 241.90139770507812, 973...   \n4       91         4  [[-11.54010009765625, 169.252197265625, 989.16...   \n\n                                                  ts  \n0  [1725487568.2857084, 1725487568.2897425, 17254...  \n1  [1725488230.4562604, 1725488230.460306, 172548...  \n2  [1725487762.2033925, 1725487762.2074304, 17254...  \n3  [1725487415.7801855, 1725487415.7842104, 17254...  \n4  [1725488018.8184955, 1725488018.8225424, 17254...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subject</th>\n      <th>chunk_id</th>\n      <th>accel</th>\n      <th>ts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>91</td>\n      <td>0</td>\n      <td>[[-16.750499725341797, 221.14559936523438, 978...</td>\n      <td>[1725487568.2857084, 1725487568.2897425, 17254...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>91</td>\n      <td>1</td>\n      <td>[[12.323999404907227, -6.38040018081665, 1004....</td>\n      <td>[1725488230.4562604, 1725488230.460306, 172548...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>91</td>\n      <td>2</td>\n      <td>[[39.14820098876953, 99.29010009765625, 989.72...</td>\n      <td>[1725487762.2033925, 1725487762.2074304, 17254...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>91</td>\n      <td>3</td>\n      <td>[[-19.905601501464844, 241.90139770507812, 973...</td>\n      <td>[1725487415.7801855, 1725487415.7842104, 17254...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>91</td>\n      <td>4</td>\n      <td>[[-11.54010009765625, 169.252197265625, 989.16...</td>\n      <td>[1725488018.8184955, 1725488018.8225424, 17254...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom scipy.signal import butter, filtfilt\n\n# Define a function to create a low-pass Butterworth filter\ndef butter_lowpass(cutoff, fs, order=4):\n    nyquist = 0.5 * fs  # Nyquist frequency\n    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\n# Apply the low-pass filter to data\ndef butter_lowpass_filter(data, cutoff, fs, order=4):\n    b, a = butter_lowpass(cutoff, fs, order)\n    return filtfilt(b, a, data)\n\n# Function to process a single subject\ndef process_subject(subject_data, cutoff_freq=45.0, sampling_rate=250):\n    # Extract accelerometer data and timestamps\n    accel_data = np.array(subject_data['accel'])\n    ts_data = np.array(subject_data['ts'])\n    \n    # Apply low-pass filter to each axis (x, y, z)\n    filtered_accel = np.array([butter_lowpass_filter(accel_data[:, i], cutoff_freq, sampling_rate) for i in range(3)]).T\n    \n    # Return processed subject data without downsampling\n    return {\n        'subject': subject_data['subject'],\n        'accel': filtered_accel.tolist(),\n        'ts': ts_data.tolist(),\n        # 'labels': subject_data['label']  # Keep original labels\n    }\n\n# Load the JSON file\nwith open('/kaggle/input/icassp-person-in-bed-track-1/test.json', 'r') as f:\n    data = json.load(f)\n\n# Process each subject independently\nprocessed_data = [process_subject(subject_data) for subject_data in data]\n\n# Save the processed data back to JSON\nwith open('LowPass_test_45.json', 'w') as f:\n    json.dump(processed_data, f)\n\nprint(\"Low-pass filtering (50 Hz) complete without downsampling.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:12:45.189799Z","iopub.execute_input":"2024-11-30T07:12:45.190244Z","iopub.status.idle":"2024-11-30T07:13:29.649448Z","shell.execute_reply.started":"2024-11-30T07:12:45.190206Z","shell.execute_reply":"2024-11-30T07:13:29.648216Z"}},"outputs":[{"name":"stdout","text":"Low-pass filtering (50 Hz) complete without downsampling.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(df_test.head())\nprint(df_test.shape)\nl = df_test['subject']\nprint(l.unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:13:35.974764Z","iopub.execute_input":"2024-11-30T07:13:35.975928Z","iopub.status.idle":"2024-11-30T07:13:36.047635Z","shell.execute_reply.started":"2024-11-30T07:13:35.975882Z","shell.execute_reply":"2024-11-30T07:13:36.046550Z"}},"outputs":[{"name":"stdout","text":"   subject  chunk_id                                              accel  \\\n0       91         0  [[-16.750499725341797, 221.14559936523438, 978...   \n1       91         1  [[12.323999404907227, -6.38040018081665, 1004....   \n2       91         2  [[39.14820098876953, 99.29010009765625, 989.72...   \n3       91         3  [[-19.905601501464844, 241.90139770507812, 973...   \n4       91         4  [[-11.54010009765625, 169.252197265625, 989.16...   \n\n                                                  ts  \n0  [1725487568.2857084, 1725487568.2897425, 17254...  \n1  [1725488230.4562604, 1725488230.460306, 172548...  \n2  [1725487762.2033925, 1725487762.2074304, 17254...  \n3  [1725487415.7801855, 1725487415.7842104, 17254...  \n4  [1725488018.8184955, 1725488018.8225424, 17254...  \n(1538, 4)\n[ 91 108 130 127 157 194 151 171 367]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"len(df_test['accel'].iloc[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:42:54.785121Z","iopub.execute_input":"2024-11-28T16:42:54.785582Z","iopub.status.idle":"2024-11-28T16:42:54.793382Z","shell.execute_reply.started":"2024-11-28T16:42:54.785545Z","shell.execute_reply":"2024-11-28T16:42:54.792126Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"2500"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"with open('/kaggle/working/LowPass_test_45.json', 'r') as f:\n    processed_data_test = json.load(f)\nprocessed_df_test = pd.DataFrame(processed_data_test)\n\nfeature_list = processed_df_test['accel'].apply(extract_wavelet_features).tolist()\n# feature_list = df_test['accel'].apply(extract_wavelet_packet_features).tolist()\nfeatures_df = pd.DataFrame(feature_list)\nfinal_df_test = features_df\n\nprint(final_df_test.head())\n\n# feature_list = df_test['accel'].apply(extract_wavelet_features).tolist()\n# # feature_list = df_test['accel'].apply(extract_wavelet_packet_features).tolist()\n# features_df = pd.DataFrame(feature_list)\n# final_df_test = features_df\n\n# print(final_df_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:13:48.000470Z","iopub.execute_input":"2024-11-30T07:13:48.000901Z","iopub.status.idle":"2024-11-30T07:14:36.707926Z","shell.execute_reply.started":"2024-11-30T07:13:48.000863Z","shell.execute_reply":"2024-11-30T07:14:36.706873Z"}},"outputs":[{"name":"stdout","text":"   cA_mean_x_level0  cA_std_x_level0  cA_max_x_level0  cA_min_x_level0  \\\n0        -32.305045         0.802624       -29.822623       -35.445396   \n1         24.847054         0.406827        26.120423        23.580158   \n2        -27.811161        35.624847       170.000804      -253.563583   \n3        -39.736626         0.619029       -38.304170       -42.560620   \n4        -21.670079         0.822990       -18.870422       -23.994127   \n\n   cA_energy_x_level0  cA_skewness_x_level0  cA_kurtosis_x_level0  \\\n0        6.578839e+05              0.065643              0.631282   \n1        3.890512e+05              0.107702              0.042736   \n2        1.286832e+06              0.832988             10.165146   \n3        9.950111e+05             -0.922166              2.123271   \n4        2.962699e+05              0.168244              0.311575   \n\n   cA_median_x_level0  cA_iqr_x_level0  cA_entropy_x_level0  ...  \\\n0          -32.358610         1.033436             6.445411  ...   \n1           24.845946         0.553732             6.445586  ...   \n2          -37.915674         8.722154             6.341548  ...   \n3          -39.708189         0.704282             6.445599  ...   \n4          -21.700852         1.029179             6.444997  ...   \n\n   cD2_std_z_level2  cD2_max_z_level2  cD2_min_z_level2  cD2_energy_z_level2  \\\n0          0.025056          0.098739         -0.102805             0.786647   \n1          0.023667          0.065946         -0.110798             0.701822   \n2          0.405226          4.601340         -6.535827           205.753006   \n3          0.018207          0.062417         -0.060452             0.415348   \n4          0.029955          0.107524         -0.118991             1.124307   \n\n   cD2_skewness_z_level2  cD2_kurtosis_z_level2  cD2_median_z_level2  \\\n0              -0.025234               0.574782            -0.000142   \n1              -0.211061               0.271620             0.000529   \n2              -2.500371              91.430730            -0.000462   \n3               0.031691               0.030680            -0.000214   \n4               0.040073               0.509043            -0.000415   \n\n   cD2_iqr_z_level2  cD2_entropy_z_level2  cD2_dominant_frequency_z_level2  \n0          0.030668              6.827653                         0.335994  \n1          0.031379              6.847192                        -0.324820  \n2          0.048225              5.534603                        -0.334397  \n3          0.024070              6.837261                         0.293695  \n4          0.037192              6.825214                         0.327215  \n\n[5 rows x 99 columns]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"final_df.shape,final_df_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:15:51.040184Z","iopub.execute_input":"2024-11-30T07:15:51.041358Z","iopub.status.idle":"2024-11-30T07:15:51.051124Z","shell.execute_reply.started":"2024-11-30T07:15:51.041296Z","shell.execute_reply":"2024-11-30T07:15:51.049987Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"((5911, 100), (1538, 99))"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Split the dataset into features and labels\n# X = final_df.drop(columns='label')\n# y = final_df['label']\n\n# Make predictions on the test set\ny_pred_test = model.predict(final_df_test)\nprint(len(y_pred_test))\n# Extract features from the test data\ntest_features = []\nchunk_ids = []\n\nfor entry in data_test:\n    chunk_id = entry['chunk_id']\n    accel_data = entry['accel']  # List of 3-axis accelerometer readings\n\n    # Extract periodic features using the defined function\n    # features = extract_periodic_features(accel_data)\n    # features['chunk_id'] = chunk_id\n    # test_features.append(features)\n    chunk_ids.append(chunk_id)\n\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'chunk_id': chunk_ids,\n    'label': y_pred_test\n})\n\n# Save the submission to a CSV file\nsubmission_file = 'Track1_lvl2_butterworth_45Hz.csv'\nsubmission_df.to_csv(submission_file, index=False)\n\nprint(f\"Submission file saved as {submission_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T07:16:02.037444Z","iopub.execute_input":"2024-11-30T07:16:02.037973Z","iopub.status.idle":"2024-11-30T07:16:02.084308Z","shell.execute_reply.started":"2024-11-30T07:16:02.037932Z","shell.execute_reply":"2024-11-30T07:16:02.083283Z"}},"outputs":[{"name":"stdout","text":"1538\nSubmission file saved as Track1_lvl2_butterworth_45Hz.csv\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"len(submission_df),len(y_pred_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T16:53:06.989878Z","iopub.execute_input":"2024-11-28T16:53:06.990291Z","iopub.status.idle":"2024-11-28T16:53:06.997946Z","shell.execute_reply.started":"2024-11-28T16:53:06.990249Z","shell.execute_reply":"2024-11-28T16:53:06.996855Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(1538, 1538)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}